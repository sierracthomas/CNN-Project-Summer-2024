{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffcdde09",
   "metadata": {},
   "source": [
    "# Part 3: Initializing a PyTorch CNN model and training it\n",
    "\n",
    "In this tutorial, we'll look at the structure of a neural network and train it. We'll also do some activities to see how long it takes to train it and see what *overtraining* looks like.\n",
    "\n",
    "Below, you'll see some familiar code from part 1 - all we are doing is loading in the data that we got previously. I could hide this code block in another file, but you'll want to make sure that `dataset` and `testset` are the same locations as you had last time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ac6811a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries and some functions that are written for you \n",
    "from handwrite_functions import *\n",
    "\n",
    "# where you want to save your dataset\n",
    "dataset = 'MY_DATASET'\n",
    "testset = 'MY_TESTSET'\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.5,), (0.5,)),\n",
    "                              ])\n",
    "\n",
    "# download training sets and test sets \n",
    "trainset = datasets.MNIST(dataset, download=False, train=True, transform=transform)\n",
    "valset = datasets.MNIST(testset, download=False, train=False, transform=transform)\n",
    "\n",
    "# load training sets and test sets in batch sizes of 64\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=64, shuffle=True)\n",
    "\n",
    "# prepare loaded data sets to iterate over\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1269e6",
   "metadata": {},
   "source": [
    "Did you successfully import the datasets? How do you know?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "45ed9fd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset MNIST\n",
       "     Number of datapoints: 60000\n",
       "     Root location: MY_DATASET\n",
       "     Split: Train\n",
       "     StandardTransform\n",
       " Transform: Compose(\n",
       "                ToTensor()\n",
       "                Normalize(mean=(0.5,), std=(0.5,))\n",
       "            ),\n",
       " Dataset MNIST\n",
       "     Number of datapoints: 10000\n",
       "     Root location: MY_TESTSET\n",
       "     Split: Test\n",
       "     StandardTransform\n",
       " Transform: Compose(\n",
       "                ToTensor()\n",
       "                Normalize(mean=(0.5,), std=(0.5,))\n",
       "            ))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset, valset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c127451",
   "metadata": {},
   "source": [
    "## Initializing a model\n",
    "\n",
    "We're going to start by initializing a model - we'll tell Python that we'd like to make a model that is structured like this: \n",
    "\n",
    "---\n",
    "$$\\text{784 input neurons to 128 neurons} \\rightarrow \\text{applies rectifier activation function } \\rightarrow \\text{128 neurons to 64 neurons} \\rightarrow \\text{applies rectifier activation function} \\rightarrow \\text{64 neurons to 10 output neurons}$$ \n",
    "---\n",
    "\n",
    "This is shown in lines 9-14 below. Does this structure match what gets printed below?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "45d4d2b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0): Neurons\n",
      "Sequential(\n",
      "  (0): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=64, out_features=10, bias=True)\n",
      "  (5): LogSoftmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# look at structure of neural network\n",
    "# play with these values to make the NN better! \n",
    "\n",
    "# input number of neurons, two \"hidden layers\", output neuron size\n",
    "input_size = 784\n",
    "hidden_sizes = [128, 64]\n",
    "output_size = 10\n",
    "\n",
    "model = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[1], output_size),\n",
    "                      nn.LogSoftmax(dim=1))\n",
    "print(\"(0): Neurons\")\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7dc778",
   "metadata": {},
   "source": [
    "In the last tutorial, our mini neural network was structured more like this: \n",
    "\n",
    "---\n",
    "$$\\text{4 input neurons} \\rightarrow \\text{sigmoid activation function } \\rightarrow \\text{1 output}$$\n",
    "---\n",
    "\n",
    "Using hidden layers lets us apply the activation function multiple times along the chain with the same dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7271b1",
   "metadata": {},
   "source": [
    "# Minimizing the \"negative log-likelihood loss\" (also known as the \"forward pass\")\n",
    "\n",
    "At the end of the steps listed above in the model's structure, there is the `LogSoftmax(dim=1)` piece. This will do something that we did in the last tutorial - the \"forward propagation\" piece.\n",
    "\n",
    "In the last tutorial, we took the derivative of our activation function to minimize the difference between predicted value and truth value. Then, we added these weights to each neuron to get closer to the \"best\" solution.\n",
    "\n",
    "$$\\text{minimize} = \\frac{d}{dx}[\\sum^{n=1}_{4}(f(\\text{inputs}) -y)^{2}]$$\n",
    "\n",
    "PyTorch does something like this - but does it in logarithms to save computing effort. Think back to algebra when we added and multiplied logs:\n",
    "\n",
    "$$\\log{(a)} + \\log{(b)} = \\log{(ab)}$$\n",
    "\n",
    "Is it easier computationally to multiply or add? Because our neural network in the last tutorial was so simple, there was no need to worry about memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5423b448",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 10]), tensor(0.0573, grad_fn=<NllLossBackward0>))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this cell is complicated but essentially... \n",
    "# maximize the likelihood of observing the data by minimizing negative log-likelihood \n",
    "# we'll let pytorch do this for us! \n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "images, labels = next(iter(trainloader))\n",
    "images = images.view(images.shape[0], -1)\n",
    "\n",
    "logps = model(images) #log probabilities\n",
    "loss = criterion(logps, labels) #calculate the NLL loss\n",
    "logps.shape, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b1d83b",
   "metadata": {},
   "source": [
    "## Backwards passes\n",
    "\n",
    "How much error does each output contribute? This function by pytorch starts at the output and propagates the error backwards to see which pathways (or output-> hidden layers -> input) contribute the most error to the final guesses. We'll calculate this here to see what it looks like, then use the function later in training. \n",
    "\n",
    "The printed output below shows us the \"changes in weights\" for each pathway (also called *node*). After this backwards pass, the NN has calculated which pathways are most error-prone and which are least error-prone. When we start training the model, we will use an \"optimizer,\" which will reduce the weights of the more error-prone pathways and increase the weights of the least error-prone pathways.\n",
    "\n",
    "We didn't do this on our last neural network tutorial! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32ceac22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before backward pass: \n",
      " None\n",
      "After backward pass: \n",
      " tensor([[ 0.0002,  0.0002,  0.0002,  ...,  0.0002,  0.0002,  0.0002],\n",
      "        [-0.0001, -0.0001, -0.0001,  ..., -0.0001, -0.0001, -0.0001],\n",
      "        [ 0.0022,  0.0022,  0.0022,  ...,  0.0022,  0.0022,  0.0022],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0025,  0.0025,  0.0025,  ...,  0.0025,  0.0025,  0.0025],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])\n",
      "Shape of the data: \n",
      " torch.Size([128, 784])\n"
     ]
    }
   ],
   "source": [
    "print(\"Before backward pass: \\n\", model[0].weight.grad)\n",
    "loss.backward()\n",
    "print(\"After backward pass: \\n\", model[0].weight.grad)\n",
    "print(\"Shape of the data: \\n\", model[0].weight.grad.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62430a75",
   "metadata": {},
   "source": [
    "# Putting everything together to train the NN\n",
    "\n",
    "This structure should look very similar to the last tutorial's structure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f155a3ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Training loss: 0.6196157126379674\n",
      "Epoch 1 - Training loss: 0.2810084430743128\n",
      "Epoch 2 - Training loss: 0.22019984526658998\n",
      "Epoch 3 - Training loss: 0.17913566346266377\n",
      "Epoch 4 - Training loss: 0.14946448202850596\n",
      "Epoch 5 - Training loss: 0.1289271490771506\n",
      "Epoch 6 - Training loss: 0.11250920073865954\n",
      "Epoch 7 - Training loss: 0.0990299687806223\n",
      "Epoch 8 - Training loss: 0.0878910678826066\n",
      "Epoch 9 - Training loss: 0.08161229755667879\n",
      "Epoch 10 - Training loss: 0.07441538415932611\n",
      "Epoch 11 - Training loss: 0.06722809490300953\n",
      "Epoch 12 - Training loss: 0.06210828351272917\n",
      "Epoch 13 - Training loss: 0.056837323304416654\n",
      "Epoch 14 - Training loss: 0.0531546590571254\n",
      "\n",
      "Training Time (in minutes) = 1.910279428958893\n"
     ]
    }
   ],
   "source": [
    "# initialize our optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.003, momentum=0.9)\n",
    "\n",
    "# define start time\n",
    "time0 = time()\n",
    "\n",
    "# How many times do we want our neural network to learn on the same data?\n",
    "# Be careful not to do it too many times, since otherwise the NN will learn the anomalies in our data\n",
    "epochs = 15\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in trainloader:\n",
    "        # Make the training data fit the same size as our given input size\n",
    "        images = images.view(images.shape[0], -1)\n",
    "    \n",
    "        # train the neural network\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(images)\n",
    "        loss = criterion(output, labels)\n",
    "        \n",
    "        #backward propagate like in the last cell\n",
    "        loss.backward()\n",
    "        \n",
    "        #have pytorch adjust to improve from backwards propagation\n",
    "        optimizer.step()\n",
    "        \n",
    "        # store \"loss.item()\" or loss percentage in \"running_loss\" variable for us to print\n",
    "        running_loss += loss.item()\n",
    "    else:\n",
    "        print(\"Epoch {} - Training loss: {}\".format(e, running_loss/len(trainloader)))\n",
    "        \n",
    "# print time it took to train\n",
    "print(\"\\nTraining Time (in minutes) =\",(time()-time0)/60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6bc06a",
   "metadata": {},
   "source": [
    "# Activities:\n",
    "\n",
    "- can you plot the training loss for each epoch? (remember to plot axes/title and save all images!)\n",
    "- can you plot how long it takes to train each epoch? (remember to plot axes/title and save all images!)\n",
    "- what happens if you don't do the backwards pass?\n",
    "- try adding another hidden layer to your NN initialization. How do you do this? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59e20d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
