{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bf3aa18",
   "metadata": {},
   "source": [
    "# Part 2: Training and testing a neural network\n",
    "\n",
    "In the last tutorial, we learned a couple different things: \n",
    "- We learned a lot of python!\n",
    "- We learned how to get data from MNIST\n",
    "- We learned what *tensors* are and that we'll need them when doing machine learning.\n",
    "- We learned that we will need a *test set* and a *training set* to teach our neural network and evaluate its performance.\n",
    "\n",
    "\n",
    "We'll take a break from PyTorch and make our own - super simple - neural network with Python. This will help us understand what's going on behind the scenes of PyTorch, which is more sophisticated than what we can do. We'll use words that you've seen before, as well as some new ones. If you don't quite understand something, you can google it or peek at the next tutorial to see! \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "In this tutorial, we'll make a neural network that looks at 1-dimensional tensors. It will give us the number $1$ if there are nonzero numbers in each column, and a $0$ if there is a zero in any of the columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ec8327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numpy is so useful!\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f079738",
   "metadata": {},
   "source": [
    "Let's start by making some *training data*. This data is just like the images from MNIST, except we will make it ourselves. For our neural network, we want to keep it super simple. In the cell below, we'll make four different data points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e906518",
   "metadata": {},
   "outputs": [],
   "source": [
    "train1 = [1,1,3]\n",
    "train2 = [2,2,1]\n",
    "train3 = [0,1,2]\n",
    "train4 = [1,0,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ad9aa9",
   "metadata": {},
   "source": [
    "We'll also need truth values (also known as *labels* or we can call it an *answer key*!). You'll notice that our 1-dimensional `labels` tensor is four columns wide. The first column entry is the *answer* to the `train1` variable. The second is the *answer* to the `train2` variable, etc. \n",
    "\n",
    "This makes sense: since our goal is to \"teach\" this NN that if a `train` variable has numbers in each column, the *answer* is 1. Do our labels match each of our training data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4a87be",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [1, 1, 0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167c3bb7",
   "metadata": {},
   "source": [
    "Now, it's time to make our \"testing data.\" We'll call it `test1`. In this tutorial, we'll only test our neural network once! But in real applications, testing datasets can be huge. Make a note of what you expect the output to be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96da551",
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = [2,2,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4940c0ab",
   "metadata": {},
   "source": [
    "Time to start - we'll organize the training sets to make the math operations easier later. (Try printing the shape of `training_set_inputs` - we've turned it into a 2-dimensional tensor!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff4b6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set_inputs = np.array([train1, train2, train3, train4])\n",
    "training_set_outputs = np.array([labels]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f324be",
   "metadata": {},
   "source": [
    "Now, we'll need to think about how a neural network \"learns.\" We'll give the neural network three *neurons* to work with. Each one will correspond to a column in the training data.\n",
    "\n",
    "When we train the neural network, we'll be multiplying these weights with the inputs - when we multiply something with zero we get zero. So, instead of giving the NN empty neurons, or neurons with zeros inside, we'll put small, random numbers inside. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebe6544",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = 2 * np.random.random((3, 1)) - 1\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc8fe74",
   "metadata": {},
   "source": [
    "Our *activation function* is an important part of machine learning. When we go back to PyTorch, we'll use a rectifier activation function - you can look up these details yourself (i.e., the different functions may be too complicated to explore in this series of tutorials)! Here, we'll use an easier one that looks like this:\n",
    "\n",
    "$$f = \\frac{1}{1 + e^{(-\\sum( \\text{inputs} * \\text{weights}))}}$$\n",
    "\n",
    "\n",
    "Remember that $\\sum$, or capital sigma, means the sum of the values following it. NumPy's `dot` function does this for us - it'll multiply the inputs and weights and sum them for us. (The activation function's coded below for you.) \n",
    "\n",
    "We'll also minimize the difference of the activation function (evaluated on all the inputs) and their truth value. \n",
    "\n",
    "$$\\text{minimize} = \\frac{d}{dx}[\\sum^{n=1}_{4}(f(\\text{inputs}) -y)^{2}]$$\n",
    "\n",
    "This is called forward propagation - when these values get as small as possible, the neuron is trained. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e61ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_function(inputs, weights):\n",
    "    # sigmoid function\n",
    "    return 1 / (1 + np.exp(-(np.dot(inputs, weights))))\n",
    "\n",
    "\n",
    "# minimize difference between predicted value and truth value\n",
    "def forwardpropagation(training_set_inputs, training_set_outputs, output):\n",
    "    # training set inputs * (f(x) - y) * gradient of the activation function\n",
    "    return np.dot(training_set_inputs.T, (training_set_outputs - output) * output * (1 - output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128e1925",
   "metadata": {},
   "source": [
    "If the activation function gives you a value with a large magnitude, then it's \"confident\" one way or another that the neuron has low error. \n",
    "\n",
    "Try using the visualizer below to see this. Make `sum_of_neuron_inputs_and_weights` extreme. If `sum_of_neuron_inputs_and_weights` is large and positive, the activation function grows quickly. If `sum_of_neuron_inputs_and_weights` is a large, negative number (like -1000), it decreases quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029e5cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sum_of_neuron_inputs_and_weights = 30\n",
    "\n",
    "x = np.linspace(-1,1,100)\n",
    "output = activation_function(x, sum_of_neuron_inputs_and_weights)\n",
    "\n",
    "plt.plot(x, output, label = \"Activation function\")\n",
    "plt.plot(x, output * (1 - output), label = \"Derivative of activation function\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8456ca0b",
   "metadata": {},
   "source": [
    "Now, we don't want the NN to \"forget\" this new weight and we want it to improve instead. Notice what happens with the orange line - the *derivative* of the activation function. If `sum_of_neuron_inputs_and_weights` is small, then the orange line looks closer to the blue line. If `sum_of_neuron_inputs_and_weights` is large, it's far away. \n",
    "\n",
    "We'll evaluate the derivative of the activation function and add/subtract that from the current weights belonging to each neuron. This way, if the neuron already has low error, subtracting a small bias from the weights won't change much. If the neuron has a high error, subtracting a larger bias from the weight can improve the NN's performance as it continues to train."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3499143c",
   "metadata": {},
   "source": [
    "---\n",
    "Whew! It's actually time to start training the NN now. In each iteration, or epoch, we'll hand our entire training dataset off to the activation function. That activation function will calculate a value to hand to each neuron. The `weights` will add or subtract a value from each neuron - small if it already looks good, and large if it doesn't look good.\n",
    "\n",
    "Go ahead and play with the epochs to see if you can get worse or better results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62789a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10000\n",
    "for epoch in range(epochs):\n",
    "    output = activation_function(training_set_inputs, weights)\n",
    "    weights += forwardpropagation(training_set_inputs, training_set_outputs, output)\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2d2152",
   "metadata": {},
   "source": [
    "And finally, time to test. Is our neural network right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e89ccaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = activation_function(np.array(test1), weights)\n",
    "\n",
    "print(f\"Testing data: {test1}\\n And our neural network thinks that the answer is {test_results}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe353edd",
   "metadata": {},
   "source": [
    "Activities: \n",
    "- What happens if you put a zero in the third place of the testing data? Why does this happen? (Hint: print the `weights` at the last step and try to see what comes out. Each position corresponds to a neuron, but we can't quite interpret the results as humans without calculating `test_results`).\n",
    "- Try training the neural network with smaller or larger epochs. How many do you need before it gets close? (note: you will need to restart the kernal each time, otherwise, if you just run that cell then you'll keep training the same NN as before.)\n",
    "- code `and`, `or`, and `xor` gates to test the NN on! Post the results in slack.\n",
    "\n",
    "\n",
    "---\n",
    "Tutorial is adapted from [here.](https://medium.com/technology-invention-and-more/how-to-build-a-simple-neural-network-in-9-lines-of-python-code-cc8f23647ca1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
